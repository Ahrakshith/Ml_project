
# ğŸš€ Student Performance Predictor: End-to-End MLOps on Google Cloud â˜ï¸

**Predicting student math scores with a production-ready, fully serverless ML system.**

## âœ¨ Project Overview

This project delivers a complete MLOps pipeline, starting from raw data analysis to a live, scalable API endpoint on Google Cloud. The goal is to **predict a student's math score** based on key attributes like gender, parental education, and test preparation.

It serves as a full-stack demonstration of MLOps best practices, CI/CD automation, and cloud-native deployment.

---

## ğŸ› ï¸ Tech Stack & Tools

| Layer | Technologies Used |
| :--- | :--- |
| **Data Processing** | Python, Pandas, NumPy |
| **Modeling** | scikit-learn (Linear Regression, Random Forest) |
| **Web Framework** | Flask (REST API) |
| **Containerization** | Docker |
| **Cloud Deployment** | Google Cloud Run, Artifact Registry |
| **Automation (CI/CD)** | Google Cloud Build |
| **Version Control** | Git, GitHub |
| **Monitoring & Logging**| Cloud Logging, Docker logs |

---

## ğŸ¯ End-to-End Workflow

The entire ML system is structured into a seven-stage, production-ready pipeline:

### 1ï¸âƒ£ Data Exploration & Model Building (Offline)

* Performed **Exploratory Data Analysis (EDA)** and deep-dive visualization in Jupyter.
* Conducted feature engineering: cleaned, analyzed relationships, handled missing values, and applied **encoding/scaling**.
* Trained and evaluated multiple **ML algorithms** (Linear Regression, Random Forest, etc.).
* Selected the best model based on the **RÂ² score** and cross-validation, then serialized it using `pickle`.
    * **Main Notebooks:** `notebook/EDA.ipynb`, `notebook/model_training.ipynb`

### 2ï¸âƒ£ Modular Pipeline Development (Code Refactoring)

The entire Jupyter workflow was refactored into a **modular, reusable Python package** to ensure reproducibility and maintainability.


src/
â”œâ”€â”€ components/
â”‚Â  Â â”œâ”€â”€ data\_ingestion.pyÂ  Â  Â  Â  \# Reads and preprocesses data source
â”‚Â  Â â”œâ”€â”€ data\_transformation.pyÂ  Â \# Handles categorical encoding, scaling, and feature engineering
â”‚Â  Â â””â”€â”€ model\_trainer.pyÂ  Â  Â  Â  Â \# Trains, evaluates, and saves the final model
â”œâ”€â”€ pipeline/
â”‚Â  Â â”œâ”€â”€ predict\_pipeline.pyÂ  Â  Â  \# Loads the production model and generates predictions
â”‚Â  Â â””â”€â”€ training\_pipeline.pyÂ  Â  Â \# Orchestrates the full end-to-end training flow
â””â”€â”€ utils.pyÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \# Helper functions (e.g., saving objects, loading models)



* âœ… Implemented robust **exception handling and logging** across all modules.

### 3ï¸âƒ£ Flask Web Application (API Layer)

Developed a lightweight **Flask web application** to serve the model for real-time predictions.

* **HTML Form Endpoint:** `/predicted_data` (Renders a form for manual user input).
* **REST API Endpoint:** `/predict` (Accepts JSON input, perfect for microservice communication).
* Includes input validation and structured error handling for a production environment.

### 4ï¸âƒ£ Containerization with Docker ğŸ³

The entire application (model, dependencies, and Flask API) was containerized into a single, production-ready Docker image to guarantee environment consistency.

* Optimized `Dockerfile` for a smaller image footprint.
* **Commands Used:**
    ```bash
    docker build -t my-ml-api:dev .
    docker run -p 8080:8080 my-ml-api:dev
    ```

### 5ï¸âƒ£ Deployment on Google Cloud Run â˜ï¸

Deployed the containerized application to **Google Cloud Run**, a fully serverless platform, providing a managed, scalable REST API endpoint.

* Used **Artifact Registry** (`ml-repo`) to store and manage the Docker images.
* Deployed the service (`my-ml-api`) for autoscaling and zero infrastructure overhead.

### 6ï¸âƒ£ CI/CD Automation with Cloud Build âš™ï¸

Implemented a complete Continuous Integration/Continuous Deployment (CI/CD) pipeline using **Google Cloud Build** and a GitHub trigger.

* Defined the CI/CD steps in `cloudbuild.yaml`.
* Every `git push` automatically triggers:
    1.  Building the new Docker image.
    2.  Pushing the image to Artifact Registry.
    3.  Deploying the updated service to Cloud Run.

---

## ğŸ’¡ Future Improvements (Roadmap)

* **Vertex AI Integration:** ğŸ” Integrate Vertex AI Pipelines for automated, scheduled model retraining and advanced MLOps features.
* **Observability:** ğŸ“Š Add monitoring dashboards using **Prometheus/Grafana** or **Cloud Monitoring** to track model drift and API health.
* **Testing:** âœ… Implement automated **unit tests** for all modular components within the CI/CD pipeline.

---

## ğŸ‘¨â€ğŸ’» Author

| **Rakshith A H** | ahrakshith122@gmail.com |
| :--- | :--- |

â­ If you found this project helpful, please give it a star!
